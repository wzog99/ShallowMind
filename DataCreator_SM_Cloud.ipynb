{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataCreator / Modeling ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing necessary modules on VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-chess\n",
    "#!pip install pystockfish\n",
    "#!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Cloud Storage for games and StockFish chess engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclient=storage.Client()\\nbucket_name = 'shallowmindfiles'\\n\\nfolders = ['ShallowMind', 'stockfish-11-mac', 'stockfish-11-linux',\\n           'stockfish-11-linux/Linux', 'stockfish-11-linux/src', 'stockfish-11-linux/tests', 'stockfish-11-linux/src/syzygy',\\n           'stockfish-11-mac/Mac', 'stockfish-11-mac/src','stockfish-11-mac/tests','stockfish-11-mac/src/syzygy']\\n\\nfor folder in folders:\\n    if not os.path.exists(folder):\\n        os.makedirs(folder)\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client=storage.Client()\n",
    "bucket_name = 'shallowmindfiles'\n",
    "\n",
    "folders = ['ShallowMind', 'stockfish-11-mac', 'stockfish-11-linux',\n",
    "           'stockfish-11-linux/Linux', 'stockfish-11-linux/src', 'stockfish-11-linux/tests', 'stockfish-11-linux/src/syzygy',\n",
    "           'stockfish-11-mac/Mac', 'stockfish-11-mac/src','stockfish-11-mac/tests','stockfish-11-mac/src/syzygy']\n",
    "\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbucket = client.get_bucket(bucket_name)\\nfor folder in folders:\\n    blobs = list(bucket.list_blobs(prefix=folder))\\n    for blob in blobs:\\n        if(not blob.name.endswith('/')):\\n            blob.download_to_filename(blob.name)\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = client.get_bucket(bucket_name)\n",
    "for folder in folders:\n",
    "    blobs = list(bucket.list_blobs(prefix=folder))\n",
    "    for blob in blobs:\n",
    "        if(not blob.name.endswith('/')):\n",
    "            blob.download_to_filename(blob.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Standard ETL tools (and chess...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chess\n",
    "import pickle\n",
    "\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Starting implementation of pyspark on data collection*\n",
    "Work in progress - trying to parallelize the data processing phase of the project to greatly enhance data collection speed.\n",
    "\n",
    "Main issue - All the moves in each game should be grouped together in th RDD\n",
    "\n",
    "Potential issue - Trying to run StockFish across multiple nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.range(12345678)\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 µs, sys: 2 µs, total: 35 µs\n",
      "Wall time: 41 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "squared = nums.map(lambda x: x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(range(12345678))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 131 ms, sys: 81.7 ms, total: 213 ms\n",
      "Wall time: 236 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_square = map(lambda x: x*x, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stockfish_eval(move):\n",
    "    engine = chess.engine.SimpleEngine.popen_uci('stockfish-11-mac/Mac/stockfish-11-64')\n",
    "    info = engine.analyse(board, chess.engine.Limit(depth=5))\n",
    "    x = str(info['score']).replace('#+', '999')\n",
    "    x = x.replace('#-', '-999')\n",
    "    score = (int(x), get_bitwise(clean_fen(board.board_fen())))\n",
    "    board.push_san(i)\n",
    "            \n",
    "    # This moves the next piece and it repeats...\n",
    "    \n",
    "    \n",
    "    #except:\n",
    "     #   board = chess.Board()\n",
    "        \n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 2, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 1046, in <lambda>\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 1046, in <lambda>\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-57e12d1d8895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msc_san_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_san_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc_san_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \"\"\"\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 2, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 1046, in <lambda>\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\", line 1046, in <lambda>\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "sc_san_rdd = sc.parallelize(list(itertools.chain.from_iterable(all_san_list)))\n",
    "type(sc_san_rdd.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d4', 'e6', 'c4', 'd5', 'Nf3']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_san_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_letter(move):\n",
    "    k = move + '-t'\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 6, localhost, executor driver): java.net.SocketException: Connection reset\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:582)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.SocketException: Connection reset\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:582)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 6, localhost, executor driver): java.net.SocketException: Connection reset\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:582)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.SocketException: Connection reset\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:582)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc_san_rdd.map(stockfish_eval).collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Reality\n",
    "### Loading data and parsing the text file for moves, results and openings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_list = [line for line in open('GMallboth.pgn')] \n",
    "\n",
    "all_moves_list = []\n",
    "i = 11\n",
    "while i <= (len(game_list)/200):           ###### NOTE: '/200' only using 1% of the data availible from grandmasters\n",
    "    all_moves_list.append(game_list[i])\n",
    "    i += 16\n",
    "    \n",
    "    \n",
    "all_results = []\n",
    "i = 13\n",
    "while i <= (len(game_list)/200):\n",
    "    all_results.append(game_list[i])\n",
    "    i += 16\n",
    "    \n",
    "all_openings = []\n",
    "i = 7\n",
    "while i <= (len(game_list)/200):\n",
    "    all_openings.append(game_list[i])\n",
    "    i += 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for getting the moves cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_move_list(move_string): #### input is a string \n",
    "    '''\n",
    "    Takes a move string that was striped from a PGN format, and removes unwanted characters and conserves SAN format.\n",
    "    \n",
    "    Input: String of plain text moves in string format\n",
    "    Output: List of SAN moves\n",
    "    \n",
    "    '''\n",
    "    testing = move_string \n",
    "    testing = testing[0:-2]  ### remove last 2 characters = '\\n'\n",
    "    testing = testing.split('. ')\n",
    "    \n",
    "    for i in range(len(testing)):\n",
    "        testing[i] = testing[i].split(' ')\n",
    "    for i in testing:\n",
    "        try:\n",
    "            del(i[2])\n",
    "        except:\n",
    "            continue\n",
    "    del(testing[0])\n",
    "\n",
    "    simplelist = list(itertools.chain.from_iterable(testing))\n",
    "    return(simplelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the function from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1378\n",
      "122111\n",
      "CPU times: user 222 ms, sys: 18.7 ms, total: 240 ms\n",
      "Wall time: 331 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['d4', 'e6', 'c4', 'd5', 'Nf3', 'Nf6', 'Nc3', 'Bb4', 'Bg5', 'h6']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "all_san_list = [get_move_list(game) for game in all_moves_list]\n",
    "print(len(all_san_list))\n",
    "len_list = []\n",
    "for game in all_san_list:\n",
    "    for move in game:\n",
    "        len_list.append(move)\n",
    "print(len(len_list))\n",
    "all_san_list[0][:10]    ### First game, first 10 moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Stockfish engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stockfish import Stockfish\n",
    "\n",
    "stockfish = Stockfish('stockfish-11-mac/Mac/stockfish-11-64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that will run in the data collection cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fen(string):\n",
    "    '''\n",
    "    Takes a fen with extraneous features\n",
    "    \n",
    "    input: plain FEN notation i.e: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR\n",
    "    output: 1x64 list - with piece abbriviation in position, open space as '1'\n",
    "    '''\n",
    "    string = string.replace('8','1'*8)\n",
    "    string = string.replace('7','1'*7)\n",
    "    string = string.replace('6','1'*6)\n",
    "    string = string.replace('5','1'*5)\n",
    "    string = string.replace('4','1'*4)\n",
    "    string = string.replace('3','1'*3)\n",
    "    string = string.replace('2','1'*2)\n",
    "    string = string.replace('1','1'*1)\n",
    "    string = string.replace('/','')\n",
    "    string_list = [i for i in string]\n",
    "    string_array = np.asarray(string_list)\n",
    "    \n",
    "    return(string_array)\n",
    "\n",
    "\n",
    "def get_bitwise(board_state):\n",
    "    '''\n",
    "    Takes a [1x64] board state and creates 6 bitwise [1x64]\n",
    "    '''\n",
    "    bs = board_state #test ---- #First posiiton tuple  #winning_position[0][0] - for i in range:\n",
    "    r_ray = np.zeros(64)\n",
    "    n_ray = np.zeros(64)\n",
    "    b_ray = np.zeros(64)\n",
    "    q_ray = np.zeros(64)\n",
    "    k_ray = np.zeros(64)\n",
    "    p_ray = np.zeros(64)\n",
    "    for i in range(64):\n",
    "        if bs[i] == 'r':\n",
    "            r_ray[i] = -1\n",
    "        if bs[i] == 'R':\n",
    "            r_ray[i] = 1 \n",
    "        \n",
    "        if bs[i] == 'b':\n",
    "            b_ray[i] = -1\n",
    "        if bs[i] == 'B':\n",
    "            b_ray[i] = 1\n",
    "        \n",
    "        if bs[i] == 'n':\n",
    "            n_ray[i] = -1\n",
    "        if bs[i] == 'N':\n",
    "            n_ray[i] = 1 \n",
    "    \n",
    "        if bs[i] == 'q':\n",
    "            q_ray[i] = -1\n",
    "        if bs[i] == 'Q':\n",
    "            q_ray[i] = 1\n",
    "    \n",
    "        if bs[i] == 'k':\n",
    "            k_ray[i] = -1\n",
    "        if bs[i] == 'K':\n",
    "            k_ray[i] = 1 \n",
    "        \n",
    "        if bs[i] == 'p':\n",
    "            p_ray[i] = -1\n",
    "        if bs[i] == 'P':\n",
    "            p_ray[i] = 1 \n",
    "    master_ray = np.array((r_ray.astype('int8'), n_ray.astype('int8'), b_ray.astype('int8'), q_ray.astype('int8'), k_ray.astype('int8'), p_ray.astype('int8')))\n",
    "    return(master_ray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the real Data Creator!\n",
    " - I am loading stockfish as the engine, and iterating through each move in each game\n",
    " - On the Virtual machine I was using, each move takes about 1 second to evaluate at depth 9\n",
    "     - Although it takes longer, in the future, I would like the depth to be deeper in order to reduce the variance amongst evaluations of the same board states\n",
    " - Embedded in the info variable is a score, which corresponds to stockfishes evaluation of the current board state\n",
    " - In the replace statements methods, I'm replacing projected checkmates with a large cp \n",
    " - It advances the next move and the process repeats\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_games = all_san_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.76 s, sys: 110 ms, total: 2.87 s\n",
      "Wall time: 7.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "list_scores = []\n",
    "list_turn = []\n",
    "\n",
    "engine = chess.engine.SimpleEngine.popen_uci('stockfish-11-mac/Mac/stockfish-11-64')\n",
    "\n",
    "for game in test_games:\n",
    "    board = chess.Board()\n",
    "    for i in game:\n",
    "        try:\n",
    "            info = engine.analyse(board, chess.engine.Limit(depth=10))\n",
    "            x = str(info['score']).replace('#+', '999') # Replaces projected checkmate term with an extremly positive \n",
    "            x = x.replace('#-', '-999')                 # Replaces projected checkmate term with an extremly negative\n",
    "            list_depth.append(info['depth'])\n",
    "            list_scores.append((int(x), get_bitwise(clean_fen(board.board_fen())))) \n",
    "        except:\n",
    "            continue   \n",
    "        #This moves the next piece and it repeats...\n",
    "        board.push_san(i)\n",
    "\n",
    "#with open('bitwise_data4.p', 'wb') as write_file:         ### Saving data to a pickle file\n",
    "    #pickle.dump(list_scores, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First boardstate evaluation by StockFish with the corresponding boardstate in 6-64 bitwise array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, array([[-1,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  1],\n",
       "        [ 0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  0],\n",
       "        [ 0,  0, -1,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1,  0,  0],\n",
       "        [ 0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "       dtype=int8))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of sequential turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_turn = []\n",
    "for game in test_games:\n",
    "    for i in range(len(game)):\n",
    "        if i % 2 == False:\n",
    "            list_turn.append(1)\n",
    "        else:\n",
    "            list_turn.append(0)\n",
    "turn_df = pd.DataFrame({'white_turn': list_turn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### n1 standard\n",
    "##### 112 seconds per 100 games: 16 cores - no GPUs\n",
    "##### 101 seconds per 100 games: 32 cores - 2 P4 GPUs\n",
    "##### 95 seconds per 100 games: 96 cores - no GPUs\n",
    "##### 100 seconds per 100 games: 96 cores - 4 P4 GPUs\n",
    "\n",
    "##### n1 high-cpu\n",
    "##### 98 seconds per 100 games 96 cores - no GPU\n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "print(len(list_turn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bitwise_data3.p', 'rb') as read_file:\n",
    "    data = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing to a dataframe we can create a model around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['cp', 'bitwise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating the pieces into independent dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rook'] = [ df['bitwise'][i][0] for i in range(len(df))]\n",
    "df['night'] = [ df['bitwise'][i][1] for i in range(len(df))]\n",
    "df['bishop'] = [ df['bitwise'][i][2] for i in range(len(df))]\n",
    "df['queen'] = [ df['bitwise'][i][3] for i in range(len(df))]\n",
    "df['king'] = [ df['bitwise'][i][4] for i in range(len(df))]\n",
    "df['pawn'] = [ df['bitwise'][i][5] for i in range(len(df))]\n",
    "df = df.drop('bitwise', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rook_cols = []\n",
    "for i in range(64):\n",
    "    rook_cols.append('rook_'+ str(i))\n",
    "    \n",
    "night_cols = []\n",
    "for i in range(64):\n",
    "    night_cols.append('night_'+ str(i))\n",
    "\n",
    "bishop_cols = []\n",
    "for i in range(64):\n",
    "    bishop_cols.append('bishop_'+ str(i))    \n",
    "    \n",
    "queen_cols = []\n",
    "for i in range(64):\n",
    "    queen_cols.append('queen_'+ str(i))\n",
    "    \n",
    "king_cols = []\n",
    "for i in range(64):\n",
    "    king_cols.append('king_'+ str(i))\n",
    "    \n",
    "pawn_cols = []\n",
    "for i in range(64):\n",
    "    pawn_cols.append('pawn_'+ str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling dataframe values and columns then recombining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = pd.DataFrame(df['rook'].values.tolist(), columns = rook_cols)\n",
    "dfn = pd.DataFrame(df['night'].values.tolist(), columns = night_cols)\n",
    "dfb = pd.DataFrame(df['bishop'].values.tolist(), columns = bishop_cols)\n",
    "dfq = pd.DataFrame(df['queen'].values.tolist(), columns = queen_cols)\n",
    "dfk = pd.DataFrame(df['king'].values.tolist(), columns = king_cols)\n",
    "dfp = pd.DataFrame(df['pawn'].values.tolist(), columns = pawn_cols)\n",
    "dfcp = df['cp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(dfcp, dfr, how='outer', right_index=True, left_index=True)\n",
    "df_merged = pd.merge(df_merged, dfn, how='outer', right_index=True, left_index=True)\n",
    "df_merged = pd.merge(df_merged, dfb, how='outer', right_index=True, left_index=True)\n",
    "df_merged = pd.merge(df_merged, dfq, how='outer', right_index=True, left_index=True)\n",
    "df_merged = pd.merge(df_merged, dfk, how='outer', right_index=True, left_index=True)\n",
    "df_merged = pd.merge(df_merged, dfp, how='outer', right_index=True, left_index=True)\n",
    "df_merged = pd.merge(turn_df, df_merged, how='outer', right_index=True, left_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving file as feather file ~8 GB file at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged.to_feather('df_merged_feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file 'df_merged_feather'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5f1751fafb1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_merge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_merged_feather'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/feather_format.py\u001b[0m in \u001b[0;36mread_feather\u001b[0;34m(path, columns, use_threads)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_use_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/feather.py\u001b[0m in \u001b[0;36mread_feather\u001b[0;34m(source, columns, use_threads)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \"\"\"\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatherReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/feather.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0m_check_pandas_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/feather.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.FeatherReader.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_reader\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._get_native_file\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.memory_map\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.MemoryMappedFile._open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file 'df_merged_feather'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "df_merge = pd.read_feather('df_merged_feather') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_turns = pd.DataFrame({'white_turn': list_turn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_merge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5dc4c24c0343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_cp_merge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_merge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_turns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_merge' is not defined"
     ]
    }
   ],
   "source": [
    "df_cp_merge = pd.merge(df_merge, df_turns, how='outer', right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cp_merge.drop('white_turn_y', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.drop('white_turn_y', axis=1, inplace=True)\n",
    "#df_cp_merge = df_cp_merge.drop('white_turn_y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cp_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cp_merge.to_feather('df_cp_feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data is almost prepped for modeling \n",
    "Thanks to the feathered file I'm now able to start my kernal from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Stockfish.__del__ at 0x136c6c5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/stockfish/models.py\", line 165, in __del__\n",
      "    self.stockfish.kill()\n",
      "AttributeError: 'Stockfish' object has no attribute 'stockfish'\n",
      "Exception ignored in: <function Stockfish.__del__ at 0x136c6c5f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/flatironschool/opt/anaconda3/lib/python3.7/site-packages/stockfish/models.py\", line 165, in __del__\n",
      "    self.stockfish.kill()\n",
      "AttributeError: 'Stockfish' object has no attribute 'stockfish'\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chess\n",
    "import pickle\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "import xgboost\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import minmax_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cp_merge = pd.read_feather('df_cp_feather') ### Corrupted? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1999 1999 10112\n"
     ]
    }
   ],
   "source": [
    "df_cp_merge2 = df_cp_merge.loc[abs(df_cp_merge['cp']) < 4000 ]\n",
    "print(df_cp_merge2['cp'].min(), df_cp_merge2['cp'].max(),(len(df_cp_merge) - len(df_cp_merge2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing target variables magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#test_white = df_cp_merge.loc[df_cp_merge['white_turn_x'] == 1]\n",
    "\n",
    "test_df = df_cp_merge2\n",
    "test_df['cp'] = test_df['cp']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df.drop('cp', axis=1)\n",
    "y = test_df['cp'] # np.array(test_df['cp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_str_dummies = pd.get_dummies(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_str_dummies2 = X_str_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_str_dummies = X_str_dummies.reset_index()\n",
    "#y = y.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *NOTE - Alternate methodology - treating features as strings; not ints* \n",
    "With poor effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_str_dummies.to_feather('X_str_dummies_feather')\n",
    "X_str_dummies = pd.read_feather('X_str_dummies_feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_str_dummies.drop(['white_turn_x_0'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y.drop('index', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=.90, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mm = MinMaxScaler()\n",
    "#y_train_scaled = mm.fit_transform(y_train.reshape(-1,1))\n",
    "#y_test_scaled = mm.transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.quantile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_set = [(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "#X, y = make_regression(n_features=385, n_samples=, )\n",
    "\n",
    "RF = RandomForestRegressor(max_depth = 200, n_estimators=1000, random_state=7)\n",
    "RF.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RF.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = (((y_pred - y_test)**2).sum())/len(y_test)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost = XGBRegressor(max_depth=6, n_estimators=385)\n",
    "XGBoost.fit(X_train, y_train, eval_set=eval_set, early_stopping_rounds = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse = np.sqrt(sklearn.metrics.mean_squared_error(y_test, y_pred))\n",
    "#rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling a Neural Network on data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    try:\n",
    "        print(len(older_model.layers[i].get_weights()[1]))\n",
    "    except:\n",
    "        print(older_model.layers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(older_model.layers[0].get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(385, input_dim= dim_input, init='normal', activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, input_dim=dim_input, init='normal', activation='elu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, init='normal', activation='elu'))\n",
    "model.compile(loss='mse', optimizer=sgd, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-7, momentum=0.7, nesterov=False)\n",
    "\n",
    "Model = Sequential()\n",
    "Model.add(Dense(1155, input_dim=385, init='normal', activation='elu'))\n",
    "Model.add(Dropout(0.1))\n",
    "Model.add(Dense(1155, init= 'normal', activation='elu'))\n",
    "Model.add(Dropout(0.1))\n",
    "Model.add(Dense(1155,init='normal', activation='elu'))\n",
    "Model.add(Dropout(0.2))\n",
    "Model.add(Dense(32, init = 'normal', activation='sigmoid'))\n",
    "Model.add(Dropout(0.2))\n",
    "Model.add(Dense(1, init = 'normal', activation='tanh'))\n",
    "\n",
    "Model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs = 5, \n",
    "                        batch_size=256, \n",
    "                        use_multiprocessing=True,\n",
    "                        validation_split=.05,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WHy does it shoot up?\n",
    "#y_pred = Model.predict(X_test)\n",
    "y_pred = older_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mm.inverse_transform(y_pred_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred, bins = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(y_test, bins = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.quantile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model.save('prelim2_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "older_model = keras.models.load_model('prelim_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "older_model.fit(X_train,\n",
    "                y_train,\n",
    "                epochs = 5, \n",
    "                batch_size=512, \n",
    "                use_multiprocessing=True,\n",
    "                validation_split=.05,\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying undersampling to minimize kurtosis effect on the target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b0a86c17bec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_cp_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_cp_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_cp_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_cp_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_cp_99\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_cp_1 = test_df.loc[test_df['cp'].abs() < .7]\n",
    "test_cp_2 = test_df.loc[(test_df['cp'].abs() > .6) & (test_df['cp'].abs() < 1.5)]\n",
    "test_cp_5 = test_df.loc[(test_df['cp'].abs() > 1.5) & (test_df['cp'].abs() < 3)]\n",
    "test_cp_10 = test_df.loc[(test_df['cp'].abs() > 3) & (test_df['cp'].abs() < 5)]\n",
    "test_cp_99 = test_df.loc[test_df['cp'].abs() > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267663 1100476 526176 207914 134868\n"
     ]
    }
   ],
   "source": [
    "print(len(test_cp_1), len(test_cp_2), len(test_cp_5), len(test_cp_10), len(test_cp_99)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### arbitrary resampling parameters\n",
    "s1 = test_cp_1.sample(n=40000)\n",
    "s2 = test_cp_2.sample(n=150000)\n",
    "s5 = test_cp_5.sample(n=140000)\n",
    "s10= test_cp_10.sample(n=len(test_cp_10))\n",
    "s99= test_cp_99.sample(n=len(test_cp_99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.concat([s1, s2, s5, s10,  s99], ignore_index=True) # s10s5,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sample_df['cp'], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = sample_df.drop('cp', axis=1)\n",
    "y_sample = sample_df['cp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19.99"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample = y_sample\n",
    "y_sample.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "y_sample_array = np.array(y_sample)\n",
    "y_sample_scaled = mm.fit_transform(y_sample_array.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_sample_scaled, bins=19);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_sample_scaled = minmax_scale(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_sample, y_sample_scaled, train_size=.92, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.02, decay=1e-9, momentum=0.6, nesterov=False)\n",
    "\n",
    "Model2 = Sequential()\n",
    "Model2.add(Dense(385, input_dim=385, activation='tanh'))\n",
    "Model2.add(Dropout(0.1))\n",
    "Model2.add(Dense(385, activation='elu'))\n",
    "Model2.add(Dropout(0.2))\n",
    "Model2.add(Dense(64, activation='elu'))\n",
    "Model2.add(Dropout(0.2))\n",
    "Model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "Model2.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 588011 samples, validate on 30948 samples\n",
      "Epoch 1/5\n",
      "588011/588011 [==============================] - 62s 105us/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.0106 - val_mse: 0.0106\n",
      "Epoch 2/5\n",
      "588011/588011 [==============================] - 60s 101us/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 3/5\n",
      "588011/588011 [==============================] - 59s 100us/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 4/5\n",
      "588011/588011 [==============================] - 59s 101us/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0105 - val_mse: 0.0105\n",
      "Epoch 5/5\n",
      "588011/588011 [==============================] - 58s 99us/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0105 - val_mse: 0.0105\n"
     ]
    }
   ],
   "source": [
    "history2 = Model2.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs = 5, \n",
    "                        batch_size=256, \n",
    "                        use_multiprocessing=True,\n",
    "                        validation_split=.05,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53823, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mm.inverse_transform(y_pred)\n",
    "y_pred = y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_pred, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model2.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply KDTRES?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = KDTree(X_str_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
